{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.constraints import unitnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "\n",
    "import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df\n",
    "\n",
    "df0_train_s = pd.read_pickle('../data/df0_train_s_ue0.pkl')\n",
    "df0_test_s = pd.read_pickle('../data/df0_test_s_ue0.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "data loaded!\n",
      "max_l should be set : 34\n"
     ]
    }
   ],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = kernel_size - 1\n",
    "    for i in xrange(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l+2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data(revs, word_idx_map, max_l, kernel_size=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, val, test = [], [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev['text'], word_idx_map, max_l, kernel_size)\n",
    "        sent = sent+list(rev['ue'])# added\n",
    "        sent.append(rev['y'])\n",
    "        if rev['split'] == 1:\n",
    "            train.append(sent)\n",
    "        elif rev['split'] == 0:\n",
    "            val.append(sent)\n",
    "        else:\n",
    "            test.append(sent)\n",
    "    train = np.array(train, dtype=np.int)\n",
    "    val = np.array(val, dtype=np.int)\n",
    "    test = np.array(test, dtype=np.int)\n",
    "    return [train, val, test]\n",
    "\n",
    "# load data\n",
    "print \"loading data...\"\n",
    "x = cPickle.load(open(\"../data/twitter-train-val-test_ue.pickle\", \"rb\"))\n",
    "revs, W, word_idx_map, vocab = x[0], x[1], x[2], x[3]\n",
    "print \"data loaded!\"\n",
    "\n",
    "print 'max_l should be set :', np.max(pd.DataFrame(revs)['num_words'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = make_idx_data(revs, word_idx_map, max_l=34, kernel_size=5)  #*** must set max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape = (4028L, 55L)\n",
      "train_Y.shape = (4028L, 2L)\n",
      "----------------------------------------------------------\n",
      "val_X.shape = (972L, 55L)\n",
      "val_Y.shape = (972L, 2L)\n",
      "----------------------------------------------------------\n",
      "test_X.shape = (5000L, 55L)\n",
      "test_Y.shape = (5000L, 2L)\n",
      "----------------------------------------------------------\n",
      "number of sentences: 10000\n",
      "vocab size: 18154\n",
      "max sentence length: 34\n",
      "W shape (18155L, 500L)\n",
      "word_idx_map length 18154\n",
      "conv_input_height 55\n",
      "conv_input_width 500\n"
     ]
    }
   ],
   "source": [
    "# Train data preparation\n",
    "N = datasets[0].shape[0]\n",
    "conv_input_width = W.shape[1]\n",
    "conv_input_height = int(datasets[0].shape[1]-1)\n",
    "\n",
    "# For each word write a word index (not vector) to X tensor\n",
    "train_X = np.zeros((N, conv_input_height), dtype=np.int)\n",
    "train_Y = np.zeros((N, 2), dtype=np.int)\n",
    "for i in xrange(N):\n",
    "    for j in xrange(conv_input_height):\n",
    "        train_X[i, j] = datasets[0][i, j]\n",
    "    train_Y[i, datasets[0][i, -1]] = 1\n",
    "    \n",
    "print 'train_X.shape = {}'.format(train_X.shape)\n",
    "print 'train_Y.shape = {}'.format(train_Y.shape)\n",
    "print '----------------------------------------------------------'\n",
    "\n",
    "\n",
    "# Validation data preparation\n",
    "Nv = datasets[1].shape[0]\n",
    "\n",
    "# For each word write a word index (not vector) to X tensor\n",
    "val_X = np.zeros((Nv, conv_input_height), dtype=np.int)\n",
    "val_Y = np.zeros((Nv, 2), dtype=np.int)\n",
    "for i in xrange(Nv):\n",
    "    for j in xrange(conv_input_height):\n",
    "        val_X[i, j] = datasets[1][i, j]\n",
    "    val_Y[i, datasets[1][i, -1]] = 1\n",
    "    \n",
    "print 'val_X.shape = {}'.format(val_X.shape)\n",
    "print 'val_Y.shape = {}'.format(val_Y.shape)\n",
    "print '----------------------------------------------------------'\n",
    "\n",
    "\n",
    "\n",
    "# Test data preparation\n",
    "Nt = datasets[2].shape[0]\n",
    "\n",
    "# For each word write a word index (not vector) to X tensor\n",
    "test_X = np.zeros((Nt, conv_input_height), dtype=np.int)\n",
    "test_Y = np.zeros((Nt, 2), dtype=np.int)\n",
    "for i in xrange(Nt):\n",
    "    for j in xrange(conv_input_height):\n",
    "        test_X[i, j] = datasets[2][i, j]\n",
    "    test_Y[i, datasets[2][i, -1]] = 1\n",
    "    \n",
    "print 'test_X.shape = {}'.format(test_X.shape)\n",
    "print 'test_Y.shape = {}'.format(test_Y.shape)\n",
    "print '----------------------------------------------------------'\n",
    "\n",
    "# Other Information\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])     #记录最长句子的单词量\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)\n",
    "print 'W shape',W.shape\n",
    "print 'word_idx_map length',len(word_idx_map)\n",
    "print 'conv_input_height',conv_input_height\n",
    "print 'conv_input_width',conv_input_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pure UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (5000, 13)\n",
      "train_y shape: (5000L,)\n",
      "test_x shape: (5000, 13)\n",
      "test_y shape: (5000L,)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "select_cols = [u'pos_num', u'neg_num', u'pos',u'ef0',u'ef1', u'ef2', u'ef3', u'ef4', u'ef5', u'ef6', u'ef7', u'ef8', u'ef9']\n",
    "\n",
    "train_x = df0_train_s[select_cols]\n",
    "train_y = np.array(df0_train_s['polarity'])\n",
    "\n",
    "test_x = df0_test_s[select_cols]\n",
    "test_y = np.array(df0_test_s['polarity'])\n",
    "\n",
    "print 'train_x shape:',train_x.shape\n",
    "print 'train_y shape:',train_y.shape\n",
    "\n",
    "print 'test_x shape:',test_x.shape\n",
    "print 'test_y shape:',test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.6524\n",
      "Testing Accuracy: 0.6512\n"
     ]
    }
   ],
   "source": [
    "model_svm = SVC()\n",
    "model_svm.fit(train_x,train_y)\n",
    "\n",
    "train_acc_ue_svm = model_svm.score(train_x,train_y)\n",
    "test_acc_ue_svm = model_svm.score(test_x,test_y)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_ue_svm\n",
    "print 'Testing Accuracy:',test_acc_ue_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.668\n",
      "Testing Accuracy: 0.659\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(train_x,train_y)\n",
    "\n",
    "train_acc_ue_lr = model_lr.score(train_x,train_y)\n",
    "test_acc_ue_lr = model_lr.score(test_x,test_y)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_ue_lr\n",
    "print 'Testing Accuracy:',test_acc_ue_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.9264\n",
      "Testing Accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(train_x,train_y)\n",
    "\n",
    "train_acc_ue_rf = model_rf.score(train_x,train_y)\n",
    "test_acc_ue_rf = model_rf.score(test_x,test_y)\n",
    "\n",
    "print 'Traing Accuracy:',model_rf.score(train_x,train_y)\n",
    "print 'Testing Accuracy:',model_rf.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v_UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (4028L, 55L)\n",
      "train_Y_label shape: (4028L,)\n",
      "test_X shape: (5000L, 55L)\n",
      "test_Y_label shape: (5000L,)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "train_Y_label = np.argmax(train_Y,axis=1)\n",
    "test_Y_label = np.argmax(test_Y,axis=1)\n",
    "\n",
    "print 'train_X shape:',train_X.shape\n",
    "print 'train_Y_label shape:',train_Y_label.shape\n",
    "\n",
    "print 'test_X shape:',test_X.shape\n",
    "print 'test_Y_label shape:',test_Y_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 1.0\n",
      "Testing Accuracy: 0.506\n"
     ]
    }
   ],
   "source": [
    "model_svm = SVC()\n",
    "model_svm.fit(train_X,train_Y_label)\n",
    "\n",
    "train_acc_w2v_svm = model_svm.score(train_X,train_Y_label)\n",
    "test_acc_w2v_svm = model_svm.score(test_X,test_Y_label)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_w2v_svm\n",
    "print 'Testing Accuracy:',test_acc_w2v_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.54865938431\n",
      "Testing Accuracy: 0.5388\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(train_X,train_Y_label)\n",
    "\n",
    "train_acc_w2v_lr = model_lr.score(train_X,train_Y_label)\n",
    "test_acc_w2v_lr = model_lr.score(test_X,test_Y_label)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_w2v_lr\n",
    "print 'Testing Accuracy:',test_acc_w2v_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.9264\n",
      "Testing Accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(train_X,train_Y_label)\n",
    "\n",
    "train_acc_w2v_rf = model_rf.score(train_X,train_Y_label)\n",
    "test_acc_w2v_rf = model_rf.score(test_X,test_Y_label)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_ue_rf\n",
    "print 'Testing Accuracy:',test_acc_ue_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s2v_UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (5000L, 513L)\n",
      "train_y shape: (5000L,)\n",
      "test_x shape: (5000L, 513L)\n",
      "test_y shape: (5000L,)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "select_cols = [u'pos_num', u'neg_num', u'pos', u'ef0', u'ef1', u'ef2', u'ef3', u'ef4', u'ef5', u'ef6', u'ef7', u'ef8', u'ef9']\n",
    "\n",
    "train_x = np.c_[df0_train_s[select_cols].values, np.array((df0_train_s['sen_vec']).tolist())]\n",
    "train_y = np.array(df0_train_s['polarity'])\n",
    "\n",
    "test_x =  np.c_[df0_test_s[select_cols].values, np.array((df0_test_s['sen_vec']).tolist())]\n",
    "test_y = np.array(df0_test_s['polarity'])\n",
    "\n",
    "print 'train_x shape:',train_x.shape\n",
    "print 'train_y shape:',train_y.shape\n",
    "\n",
    "print 'test_x shape:',test_x.shape\n",
    "print 'test_y shape:',test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.6588\n",
      "Testing Accuracy: 0.659\n"
     ]
    }
   ],
   "source": [
    "model_svm = SVC()\n",
    "model_svm.fit(train_x,train_y)\n",
    "\n",
    "train_acc_s2v_svm = model_svm.score(train_x,train_y)\n",
    "test_acc_s2v_svm = model_svm.score(test_x,test_y)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_s2v_svm\n",
    "print 'Testing Accuracy:',test_acc_s2v_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.7352\n",
      "Testing Accuracy: 0.7306\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(train_x,train_y)\n",
    "\n",
    "train_acc_s2v_lr = model_lr.score(train_x,train_y)\n",
    "test_acc_s2v_lr = model_lr.score(test_x,test_y)\n",
    "\n",
    "print 'Traing Accuracy:',train_acc_s2v_lr\n",
    "print 'Testing Accuracy:',test_acc_s2v_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 0.9886\n",
      "Testing Accuracy: 0.6642\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(train_x,train_y)\n",
    "\n",
    "train_acc_s2v_rf = model_rf.score(train_x,train_y)\n",
    "test_acc_s2v_rf = model_rf.score(test_x,test_y)\n",
    "\n",
    "print 'Traing Accuracy:',model_rf.score(train_x,train_y)\n",
    "print 'Testing Accuracy:',model_rf.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['ue']*3 + ['w2v_ue']*3 + ['s2v_ue']*3\n",
    "method = ['svm','lr','rf']*3\n",
    "train = [train_acc_ue_svm,train_acc_ue_lr,train_acc_ue_rf,train_acc_w2v_svm,train_acc_w2v_lr,train_acc_w2v_rf,train_acc_s2v_svm,train_acc_s2v_lr,train_acc_s2v_rf]\n",
    "test = [test_acc_ue_svm,test_acc_ue_lr,test_acc_ue_rf,test_acc_w2v_svm,test_acc_w2v_lr,test_acc_w2v_rf,test_acc_s2v_svm,test_acc_s2v_lr,test_acc_s2v_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stat = pd.DataFrame({'data':data,'method':method,'train_acc':train,'test_acc':test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>method</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ue</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ue</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.6590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ue</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>0.6120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w2v_ue</td>\n",
       "      <td>svm</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w2v_ue</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.548659</td>\n",
       "      <td>0.5388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>w2v_ue</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.983863</td>\n",
       "      <td>0.6104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s2v_ue</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.658800</td>\n",
       "      <td>0.6590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s2v_ue</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.735200</td>\n",
       "      <td>0.7306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s2v_ue</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.988600</td>\n",
       "      <td>0.6642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data method  train_acc  test_acc\n",
       "0      ue    svm   0.652400    0.6512\n",
       "1      ue     lr   0.668000    0.6590\n",
       "2      ue     rf   0.926400    0.6120\n",
       "3  w2v_ue    svm   1.000000    0.5060\n",
       "4  w2v_ue     lr   0.548659    0.5388\n",
       "5  w2v_ue     rf   0.983863    0.6104\n",
       "6  s2v_ue    svm   0.658800    0.6590\n",
       "7  s2v_ue     lr   0.735200    0.7306\n",
       "8  s2v_ue     rf   0.988600    0.6642"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stat[['data','method','train_acc','test_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stat.to_pickle('../data/df_stat.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
