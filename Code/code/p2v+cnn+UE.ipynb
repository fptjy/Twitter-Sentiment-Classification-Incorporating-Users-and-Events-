{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedLineDocument\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.constraints import unitnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "\n",
    "import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"../data/train_text-norm.csv\"\n",
    "test_file = \"../data/test_text-norm.csv\"\n",
    "all_file = \"../data/all_text-norm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [train_file, test_file]\n",
    "with open(all_file, 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_file 1600000\n",
      "test_file 359\n",
      "all_file 1600359\n"
     ]
    }
   ],
   "source": [
    "print 'train_file', sum(1 for line in open(train_file))\n",
    "print 'test_file', sum(1 for line in open(test_file))\n",
    "print 'all_file', sum(1 for line in open(all_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = TaggedLineDocument(all_file)\n",
    "\n",
    "doc2vec_model = gensim.models.Doc2Vec(alpha=0.025, min_alpha=0.025) \n",
    "doc2vec_model.build_vocab(documents)\n",
    "\n",
    "for epoch in range(1):   # 需要增大变得更加精确\n",
    "    doc2vec_model.train(documents)\n",
    "    doc2vec_model.alpha -= 0.002  # decrease the learning rate\n",
    "    doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "doc2vec_model.save(\"../model/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open(all_file))\n",
    "\n",
    "doc_vectors = []\n",
    "for i in range(num_lines):\n",
    "    doc_vectors.append(doc2vec_model.docvecs[i])\n",
    "doc_vectors = np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_vectors (1600359L, 300L)\n"
     ]
    }
   ],
   "source": [
    "print 'doc_vectors',doc_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p2c parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = doc_vectors[:sum(1 for line in open(train_file)),]\n",
    "test_data = doc_vectors[sum(1 for line in open(train_file)):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data (1600000L, 300L)\n",
      "test_data (359L, 300L)\n"
     ]
    }
   ],
   "source": [
    "print 'train_data',train_data.shape\n",
    "print 'test_data',test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ue parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0_train_s_p2v = pd.read_pickle('../data/df0_train_s_p2v.pkl')\n",
    "df0_test_s_p2v = pd.read_pickle('../data/df0_test_s_p2v.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df0_train_s_p2v (1600000, 14)\n",
      "df0_test_s_p2v (359, 14)\n"
     ]
    }
   ],
   "source": [
    "print 'df0_train_s_p2v',df0_train_s_p2v.shape\n",
    "print 'df0_test_s_p2v',df0_test_s_p2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 2],\n",
       "       [3, 4, 3, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = np.c_[train_data,df0_train_s_p2v.values[,1:]]\n",
    "train_y = df0_train_s_p2v.values[,0]\n",
    "\n",
    "test_x = np.c_[test_data,df0_test_s_p2v.values[,1:]]\n",
    "test_y = df0_test_s_p2v.values[,0]\n",
    "\n",
    "print 'train_x shape:',train_x.shape\n",
    "print 'train_y shape:',train_y.shape\n",
    "\n",
    "print 'test_x shape:',test_x.shape\n",
    "print 'test_y shape:',test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面未执行完"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def get_test_acc(test_x_cat,test_y,model):\n",
    "    p1 = model.predict_proba(test_x_cat)\n",
    "    return np.mean(test_y==(np.argmax(p1,axis=1)))\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_cat = np_utils.to_categorical(train_y)\n",
    "test_y_cat = np_utils.to_categorical(test_y)\n",
    "\n",
    "train_x_cat = train_x.reshape(train_x.shape[0],1,train_x.shape[1],1)\n",
    "test_x_cat = test_x.reshape(test_x.shape[0],1,test_x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd = SGD()\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Convolution2D(10, 5, 1,border_mode=\"valid\",activation=\"relu\",input_shape=(1, train_x.shape[1], 1)))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(10))\n",
    "model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(2))\n",
    "model_cnn.add(Activation('softmax'))\n",
    "\n",
    "model_cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model_cnn.fit(train_x_cat,train_y_cat,nb_epoch=300,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_test_acc(test_x_cat,test_y,model_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
