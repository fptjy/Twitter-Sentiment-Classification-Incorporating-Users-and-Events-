{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.constraints import unitnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "\n",
    "import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load basic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入原始数据\n",
    "df0 = pd.read_csv('../data/training.1600000.processed.noemoticon.csv',header=None,names=['polarity','id','date','query','name','text'])\n",
    "df1 = pd.read_csv(\"../data/train_text-norm.csv\",header=None,names=['text'])\n",
    "df0['text'] = df1['text']\n",
    "\n",
    "np.random.seed(0)\n",
    "# 划分训练集测试集\n",
    "sss = StratifiedShuffleSplit(df0['polarity'], 3, test_size=0.5, random_state=0)\n",
    "for train_index, test_index in sss:\n",
    "    df0_train, df0_test = df0.iloc[train_index,:], df0.iloc[test_index,:]\n",
    "    \n",
    "df0_train_s = df0_train.sample(5000,random_state=0)\n",
    "df0_test_s = df0_test.sample(5000,random_state=0)\n",
    "\n",
    "df0_train_s.polarity[df0_train_s.polarity==4]=1\n",
    "df0_test_s.polarity[df0_test_s.polarity==4]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build user feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# document matrix\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(np.append(df0_train_s['text'].values,df0_test_s['text'].values))\n",
    "# vocab\n",
    "vocab = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 连接外部词库\n",
    "positive_words = pd.read_table('../data/positive-words.txt',header=None).values.ravel()\n",
    "negtive_words = pd.read_table('../data/negative-words.txt',header=None).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_words(s,option_words):\n",
    "    return len(set(s.split()) & set(option_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 means negative, 1 means positive\n",
    "# train data\n",
    "df0_train_s['pos_num'] = df0_train_s.text.apply(lambda s:get_number_words(s,positive_words))\n",
    "df0_train_s['neg_num'] = df0_train_s.text.apply(lambda s:get_number_words(s,negtive_words))\n",
    "df0_train_s['pos'] = df0_train_s['pos_num']-df0_train_s['neg_num']\n",
    "# test data\n",
    "df0_test_s['pos_num'] = df0_test_s.text.apply(lambda s:get_number_words(s,positive_words))\n",
    "df0_test_s['neg_num'] = df0_test_s.text.apply(lambda s:get_number_words(s,negtive_words))\n",
    "df0_test_s['pos'] = df0_test_s['pos_num']-df0_test_s['neg_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build event feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: day happy good today hope going morning great weather night better birthday soon tomorrow nice weekend looking time ll\n",
      "Topic 1: twitter know just thanks new got like don did won tweets hey doesn work lol sure sorry working need\n",
      "Topic 2: like know lol don just good ll want say did thanks sorry really oh think haha yeah people im\n",
      "Topic 3: http com bit twitpic ly tinyurl www th thanks tweet love plurk new blog miss pic says live check\n",
      "Topic 4: quot love new just watching haha amp like wait song hope time good best girl read listening awesome movie\n",
      "Topic 5: just good like going im lol amp think know watching eat home dont movie time got food tonight let\n",
      "Topic 6: today just ve time oh amp got fun game day school missed guess old doing play week playing going\n",
      "Topic 7: love lt really im miss lol amp thank just haha oh need night come sad going wish awesome tonight\n",
      "Topic 8: just got bad like today feel think sick really hate right little sorry want hurts head need phone hair\n",
      "Topic 9: work going day tomorrow home time sleep morning night today bed good ready getting school week im hours really\n"
     ]
    }
   ],
   "source": [
    "# 设定lda参数\n",
    "n_topics = 10\n",
    "n_iter = 1500\n",
    "\n",
    "## 建立模型\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter, random_state=1)\n",
    "lda_model.fit(tf)  # fit train data tf  \n",
    "## 输出模型的topic分类参数\n",
    "topic_word = lda_model.topic_word_ \n",
    "n_top_words = 20\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model.doc_topic的大小是： (10000L, 10L)\n"
     ]
    }
   ],
   "source": [
    "print 'lda_model.doc_topic的大小是：',lda_model.doc_topic_.shape\n",
    "# event feature for train data\n",
    "ef_train = (lda_model.doc_topic_)[:len(df0_train_s),]\n",
    "# event feature for test data\n",
    "ef_test = (lda_model.doc_topic_)[len(df0_train_s):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef_train_df\n",
    "ef_train_df = pd.DataFrame(ef_train)\n",
    "ef_train_df.columns = [\"ef\" + str(i) for i in xrange(0,10)] #***\n",
    "ef_train_df.index=df0_train_s.index\n",
    "\n",
    "# ef_test_df\n",
    "ef_test_df = pd.DataFrame(ef_test)\n",
    "ef_test_df.columns = [\"ef\" + str(i) for i in xrange(0,10)] #***\n",
    "ef_test_df.index=df0_test_s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df0\n",
    "df0_train_s = pd.concat([df0_train_s,ef_train_df],axis=1)\n",
    "df0_test_s = pd.concat([df0_test_s,ef_test_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v_ue feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_cols = [u'pos_num', u'neg_num', u'pos',u'ef0',u'ef1', u'ef2', u'ef3', u'ef4', u'ef5', u'ef6', u'ef7', u'ef8', u'ef9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义预处理函数\n",
    "def build_data_train_test(data_train, data_test, train_ratio = 0.8, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into train and test sets.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    # Pre-process train data set\n",
    "    for i in xrange(data_train.shape[0]):\n",
    "        line = data_train['text'].iloc[i]\n",
    "        y = data_train['polarity'].iloc[i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'ue':data_train.iloc[i,][select_cols].values,\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        revs.append(datum)\n",
    "        \n",
    "    # Pre-process test data set\n",
    "    for i in xrange(data_test.shape[0]):\n",
    "        line = data_test['text'].iloc[i]\n",
    "        y = data_test['polarity'].iloc[i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'ue':data_test.iloc[i,][select_cols].values,\n",
    "                  'split': -1}\n",
    "        revs.append(datum)\n",
    "        \n",
    "    return revs, vocab\n",
    "\n",
    "    \n",
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in xrange(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def build_dict(model_word2vec,vocab):\n",
    "    w2v = {}\n",
    "    for i in model_word2vec.index2word:\n",
    "        if i in vocab:\n",
    "            w2v[i] = model_word2vec[i]\n",
    "    return (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of sentences: 10000\n",
      "vocab size: 18154\n",
      "max sentence length: 34\n",
      "loading word2vec vectors... word2vec loaded!\n",
      "num words already in word2vec: 13440\n",
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "#w2v_file = '../GoogleNews-vectors-negative300.bin'  #谷歌w2v\n",
    "revs, vocab = build_data_train_test(df0_train_s, df0_test_s, train_ratio=0.8, clean_string=True)  #获得文本信息及词汇信息\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])     #记录最长句子的单词量\n",
    "print 'data loaded!'\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)\n",
    "print 'loading word2vec vectors...',\n",
    "\n",
    "# w2v构建\n",
    "model_word2vec = gensim.models.Word2Vec.load(\"../model/model_word2vec.model\")\n",
    "w2v = build_dict(model_word2vec,vocab)\n",
    "print 'word2vec loaded!'\n",
    "print 'num words already in word2vec: ' + str(len(w2v))\n",
    "\n",
    "add_unknown_words(w2v, vocab,k=500) #***\n",
    "W, word_idx_map = get_W(w2v,k=500)  #****\n",
    "\n",
    "# 数据存储*****\n",
    "cPickle.dump([revs, W, word_idx_map, vocab], open('../data/twitter-train-val-test_ue.pickle', 'wb')) #****\n",
    "print 'dataset created!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s2v_ue feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建sen_vec特征\n",
    "def AvgWord2Vec(sentence, vec_size=500):\n",
    "    global model_word2vec\n",
    "    vector = np.zeros(vec_size)\n",
    "    num = len(sentence)\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            vector += model_word2vec[word]\n",
    "        except KeyError:\n",
    "            num -= 1\n",
    "    if num > 0:\n",
    "        return (vector / num)\n",
    "    else:\n",
    "        return (vector)\n",
    "\n",
    "model_word2vec = gensim.models.Word2Vec.load(\"../model/model_word2vec.model\")\n",
    "\n",
    "df0_train_s['sen_vec'] = map(lambda x: AvgWord2Vec(x)  ,map(lambda x: x.split(),df0_train_s['text']))\n",
    "df0_test_s['sen_vec'] = map(lambda x: AvgWord2Vec(x)  ,map(lambda x: x.split(),df0_test_s['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v_ue保存在：'../data/twitter-train-val-test_ue.pickle'\n",
    "\n",
    "s2v_ue及其对应的原始数据保存在：'../data/df0_train_s_ue0.pkl'和'../data/df0_test_s_ue0.pkl'中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0_train_s.to_pickle('../data/df0_train_s_ue0.pkl')\n",
    "df0_test_s.to_pickle('../data/df0_test_s_ue0.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
