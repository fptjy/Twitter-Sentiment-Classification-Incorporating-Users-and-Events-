{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.constraints import unitnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from keras import callbacks\n",
    "\n",
    "import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load basic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\younggy\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\younggy\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# 载入训练数据\n",
    "df0_train_s = pd.read_csv('../data/training.1600000.processed.noemoticon.csv',header=None,names=['polarity','id','date','query','name','text'])\n",
    "df1_train_s = pd.read_csv(\"../data/train_text-norm.csv\",header=None,names=['text'])\n",
    "df0_train_s['text'] = df1_train_s['text']\n",
    "df0_train_s.polarity[df0_train_s.polarity==4]=1\n",
    "\n",
    "# 载入测试数据\n",
    "df0_test_s = pd.read_csv('../data/testdata.csv',header=None,names=['polarity','text'])\n",
    "df1_test_s = pd.read_csv(\"../data/test_text-norm.csv\",header=None,names=['text'])\n",
    "df0_test_s['text'] = df1_test_s['text']\n",
    "df0_test_s.polarity[df0_test_s.polarity==4]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build user feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# document matrix\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(np.append(df0_train_s['text'].values,df0_test_s['text'].values))\n",
    "# vocab\n",
    "vocab = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 连接外部词库\n",
    "positive_words = pd.read_table('../data/positive-words.txt',header=None).values.ravel()\n",
    "negtive_words = pd.read_table('../data/negative-words.txt',header=None).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_words(s,option_words):\n",
    "    return len(set(s.split()) & set(option_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 means negative, 1 means positive\n",
    "# train data\n",
    "df0_train_s['pos_num'] = df0_train_s.text.apply(lambda s:get_number_words(s,positive_words))\n",
    "df0_train_s['neg_num'] = df0_train_s.text.apply(lambda s:get_number_words(s,negtive_words))\n",
    "df0_train_s['pos'] = df0_train_s['pos_num']-df0_train_s['neg_num']\n",
    "# test data\n",
    "df0_test_s['pos_num'] = df0_test_s.text.apply(lambda s:get_number_words(s,positive_words))\n",
    "df0_test_s['neg_num'] = df0_test_s.text.apply(lambda s:get_number_words(s,negtive_words))\n",
    "df0_test_s['pos'] = df0_test_s['pos_num']-df0_test_s['neg_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build event feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_iter = 1   #****这里为了加快调整了参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: just good day like http quot today going love work time got lol im com know don amp really\n",
      "Topic 1: just good day like http quot today going work love got lol time com know im really don amp\n",
      "Topic 2: just good day like quot http today love work going lol got time im com amp know really don\n",
      "Topic 3: just good day like http quot today work love going got lol time com know don im really amp\n",
      "Topic 4: just day good like http quot today love work going got lol time know im com don really amp\n",
      "Topic 5: just good day like http quot today work going love got lol time com know really don im amp\n",
      "Topic 6: just good day like quot http today work going got love time lol im com really know don amp\n",
      "Topic 7: just good day like http quot today work going love got lol time know im really com amp don\n",
      "Topic 8: just good day like quot http work today love going got lol time im know com don really amp\n",
      "Topic 9: just day good like quot http today work love going got lol time com know don im really amp\n"
     ]
    }
   ],
   "source": [
    "# 设定lda参数\n",
    "n_topics = 10\n",
    "n_iter = 1   #****这里为了加快调整了参数\n",
    "\n",
    "## 建立模型\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter, random_state=1)\n",
    "lda_model.fit(tf)  # fit train data tf  \n",
    "## 输出模型的topic分类参数\n",
    "topic_word = lda_model.topic_word_ \n",
    "n_top_words = 20\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_model.doc_topic的大小是： (1600359L, 10L)\n"
     ]
    }
   ],
   "source": [
    "print 'lda_model.doc_topic的大小是：',lda_model.doc_topic_.shape\n",
    "# event feature for train data\n",
    "ef_train = (lda_model.doc_topic_)[:len(df0_train_s),]\n",
    "# event feature for test data\n",
    "ef_test = (lda_model.doc_topic_)[len(df0_train_s):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef_train_df\n",
    "ef_train_df = pd.DataFrame(ef_train)\n",
    "ef_train_df.columns = [\"ef\" + str(i) for i in xrange(0,10)] #***\n",
    "ef_train_df.index=df0_train_s.index\n",
    "\n",
    "# ef_test_df\n",
    "ef_test_df = pd.DataFrame(ef_test)\n",
    "ef_test_df.columns = [\"ef\" + str(i) for i in xrange(0,10)] #***\n",
    "ef_test_df.index=df0_test_s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df0\n",
    "df0_train_s = pd.concat([df0_train_s,ef_train_df],axis=1)\n",
    "df0_test_s = pd.concat([df0_test_s,ef_test_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ue for p2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_cols = [u'polarity',u'pos_num', u'neg_num', u'pos',u'ef0',u'ef1', u'ef2', u'ef3', u'ef4', u'ef5', u'ef6', u'ef7', u'ef8', u'ef9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df0_train_s_p2v (1600000, 14)\n",
      "df0_test_s_p2v (359, 14)\n"
     ]
    }
   ],
   "source": [
    "df0_train_s_p2v = df0_train_s[select_cols]\n",
    "df0_test_s_p2v = df0_test_s[select_cols]\n",
    "\n",
    "print 'df0_train_s_p2v',df0_train_s_p2v.shape\n",
    "print 'df0_test_s_p2v',df0_test_s_p2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0_train_s_p2v.to_pickle('../data/df0_train_s_p2v.pkl')\n",
    "df0_test_s_p2v.to_pickle('../data/df0_test_s_p2v.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的我没执行，数据太多啦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v_ue feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_cols = [u'pos_num', u'neg_num', u'pos',u'ef0',u'ef1', u'ef2', u'ef3', u'ef4', u'ef5', u'ef6', u'ef7', u'ef8', u'ef9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义预处理函数\n",
    "def build_data_train_test(data_train, data_test, train_ratio = 0.8, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into train and test sets.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    # Pre-process train data set\n",
    "    for i in xrange(data_train.shape[0]):\n",
    "        line = data_train['text'].iloc[i]\n",
    "        y = data_train['polarity'].iloc[i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'ue':data_train.iloc[i,][select_cols].values,\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        revs.append(datum)\n",
    "        \n",
    "    # Pre-process test data set\n",
    "    for i in xrange(data_test.shape[0]):\n",
    "        line = data_test['text'].iloc[i]\n",
    "        y = data_test['polarity'].iloc[i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'ue':data_test.iloc[i,][select_cols].values,\n",
    "                  'split': -1}\n",
    "        revs.append(datum)\n",
    "        \n",
    "    return revs, vocab\n",
    "\n",
    "    \n",
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in xrange(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def build_dict(model_word2vec,vocab):\n",
    "    w2v = {}\n",
    "    for i in model_word2vec.index2word:\n",
    "        if i in vocab:\n",
    "            w2v[i] = model_word2vec[i]\n",
    "    return (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "#w2v_file = '../GoogleNews-vectors-negative300.bin'  #谷歌w2v\n",
    "revs, vocab = build_data_train_test(df0_train_s, df0_test_s, train_ratio=0.8, clean_string=True)  #获得文本信息及词汇信息\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])     #记录最长句子的单词量\n",
    "print 'data loaded!'\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)\n",
    "print 'loading word2vec vectors...',\n",
    "\n",
    "# w2v构建\n",
    "model_word2vec = gensim.models.Word2Vec.load(\"../model/model_word2vec.model\")\n",
    "w2v = build_dict(model_word2vec,vocab)\n",
    "print 'word2vec loaded!'\n",
    "print 'num words already in word2vec: ' + str(len(w2v))\n",
    "\n",
    "add_unknown_words(w2v, vocab,k=500) #***\n",
    "W, word_idx_map = get_W(w2v,k=500)  #****\n",
    "\n",
    "# 数据存储*****\n",
    "cPickle.dump([revs, W, word_idx_map, vocab], open('../data/twitter-train-val-test_ue_big.pickle', 'wb')) #****\n",
    "print 'dataset created!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s2v_ue feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建sen_vec特征\n",
    "def AvgWord2Vec(sentence, vec_size=500):\n",
    "    global model_word2vec\n",
    "    vector = np.zeros(vec_size)\n",
    "    num = len(sentence)\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            vector += model_word2vec[word]\n",
    "        except KeyError:\n",
    "            num -= 1\n",
    "    if num > 0:\n",
    "        return (vector / num)\n",
    "    else:\n",
    "        return (vector)\n",
    "\n",
    "model_word2vec = gensim.models.Word2Vec.load(\"../model/model_word2vec.model\")\n",
    "\n",
    "df0_train_s['sen_vec'] = map(lambda x: AvgWord2Vec(x)  ,map(lambda x: x.split(),df0_train_s['text']))\n",
    "df0_test_s['sen_vec'] = map(lambda x: AvgWord2Vec(x)  ,map(lambda x: x.split(),df0_test_s['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v_ue保存在：'../data/twitter-train-val-test_ue_big.pickle'\n",
    "\n",
    "s2v_ue及其对应的原始数据保存在：'../data/df0_train_s_ue0_big.pkl'和'../data/df0_test_s_ue0_big.pkl'中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0_train_s.to_pickle('../data/df0_train_s_ue0_big.pkl')\n",
    "df0_test_s.to_pickle('../data/df0_test_s_ue0_big.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
